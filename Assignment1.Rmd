---
title: "Assignment1: RMarkdown Homework"
author: "Egecan Esen"
date: "5/8/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

I am Egecan Esen, 23 years old and planning to graduate from Boğaziçi University next semester. Before I leave this school, I would like to be able to fully use a programming language like R or Python for data analysis. I already think I am able to make some simple data analyses, however I think a structured learning experience from people who are experienced in this field will help me immensely in this journey.
I am currently working in Sakarya at Gizemfrit. It’s a 2 month long internship at the planning department. It's for the production internship that industrial engineering students are required to do.
I got an offer to work at an energy trading company next semester, which will also require data analysis skills and experience in coding. 
My linkedin profile is [here](https://www.linkedin.com/in/egecan-esen/).

[useR! 2020: Continuous integration will save your team and your packages](https://www.youtube.com/watch?v=lP0FjbNvz-Q&list=LL6rjJ5W7q1xAkiAHKAdiFMQ&index=3&t=0s)

In this video, ***Sebastian Engel-Wolf*** talks about continuous integration while working on collaborative projects in R. Continuous integration is necessary when the code is being edited by multiple people and the changes that are made could be confusing. He gives an example use-case for the statistics of a new drug that is being tested where he uses an 80% quantile. This is different than what statisticians usually use, which is 99% quantile, and he says that this could cause problems when someone else is working on this code. His initial suggestion to solve this problem is to wrap his code in an R package. He says that this is way better to see the code in the summary function. Another suggestion he has is to put the code in version control, upload it to github. This means that a coworker who is working on this project would have to create a new branch to make changes and this would make the mistake apparent and trackable, if there is any. He also puts some checks in R code that fails if there are any changes that the original coder would have liked to keep in the code. Using google cloud to see these checks also enables the coworker to easily see the mistakes he made and enables him to quickly change them.

[Visualizing Protests for Racial Justice with ggplot and gganimate](https://www.r-bloggers.com/visualizing-protests-for-racial-justice-with-ggplot-and-gganimate/)

The racial justice protests that happened in United States of America after the murders of several African Americans this year such as George Floyd and Breonna Taylor peaked my interest due to their sheer size and nature. In this blog post, ***Ignacio Sarmiento Barbieri*** visualizes these protests and their magnitude. (He uses data provided by countlove.org) He aims to contribute insights for the ongoing discussion about racial discrimination. He first loads packages that will be necessary for the analysis and visualization procedure. He filters the data for the racial injustice protests only, since they are what he is interested in. The first and simples analysis he makes is counting the number of protests and plotting them according to their dates. He also plots the most recent police murders, which shows the number of protests that come after these are the most we see in the last couple of years. There are spikes here and there, and he adds other events (such as MLK day and Charlottesville “rally”). After adding these points, it is apparent that the racial protests follows these events very closely. He then uses gganimate to show the location and sizes of these protests on the US map.

[Three Strategies for Working with Big Data in R](https://rviews.rstudio.com/2019/07/17/3-big-data-strategies-for-r/)

***Alex Gold***, an RStudio solutions engineer, talks about strategies for working with big data.  He says the biggest issue that a programmer faces when he tries to use R for big data is the fact that R only uses data that can fit into your computer’s memory, RAM. He says that the computer’s RAM needs to be 2-3 times the size of the data. However, he says there are effective strategies for working with Big Data. The first strategy is “downsampling” the data. It greatly increases speed,  is an easy way to prototype some of the data set and fine tune the parameters and enables the programmer to fully use all the packages he wants. However, it is hard to ensure that the sample is valid. The programmer needs to be sure to have a strategy for scaling the prototype. And sometimes, downsampling makes it very hard to get some totals on the data. Another strategy he talks about is “chunk and pull” which splits the data into logical chunks which will be pulled individually. This helps to achieve parallelization and uses the full data set. However, the data needs to be separable into chunks. Pulling all the data could be time intensive and the data could get stale without refreshes from the database. The last strategy Alex talks about is “push compute to data”. This is a simple method where the dataset is compressed while getting pulled from the database before getting pulled into R. It’s advantages is using less info for the same calculation, meaning less transfer. However database speed could be slow or the database the programmer is using might not support some operations. Alex gives an example for each of these strategies at the end.

[Evolve your own beats: automatically generating music via algorithms](https://www.r-bloggers.com/evolve-your-own-beats-automatically-generating-music-via-algorithms/)

***Vik Paruchuri*** talks about a strategy that evolves new music from existing pieces of music. He argues that it is easy to acquire music that is categorized, which enables him to teach a computer to categorize music automatically. He believes this teaching will automatically give the programmer a musical quality assessment tool. Thus, by generating new music and using this assessment tool, he says that it is easy to save the best tracks that are generated. The main problem with this is using existing songs, which only mixes together known sounds and cannot create music the way a human does.He firstly downloads 500+ electronic and classical music samples from the internet and converts these mp3 files to an easier format to work with. Then he builds the MQAT which decides if the music is classical or electronic. The new spliced tracks are “good” if the result of the MQAT is close to the signature of electronic or classical songs. The major problems with this method, which require further improvements are the transitions, better splicing of the patterns and create a “genetic algorithm” approach to optimize the fitness of the output track.


